"""
PTT çˆ¬èŸ² Celery ä»»å‹™
å°‡åŸæœ‰çš„ PTT çˆ¬èŸ²é‚è¼¯è½‰æ›ç‚ºåˆ†æ•£å¼ä»»å‹™
"""
import collections
import datetime
import os
import random
import time
import urllib.parse
import pandas as pd
import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent
from sqlalchemy import create_engine, BigInteger, Column, Date, Float, MetaData, String, Table, Text, Integer
from sqlalchemy.dialects.mysql import insert
from crawler.config import (
    MYSQL_ACCOUNT, MYSQL_HOST, MYSQL_PASSWORD, MYSQL_PORT, MYSQL_DATABASE,
    PTT_BOARD, PTT_DELAY_MIN, PTT_DELAY_MAX, PTT_TIMEOUT
)
from crawler.worker import app


# PTT çˆ¬èŸ²ç›¸é—œçš„é¡åˆ¥å’Œå‡½æ•¸ (ç§»æ¤è‡ªåŸæœ¬çš„ ptt_crawler.py)

# è‡ªå®šç¾©ä¾‹å¤–é¡åˆ¥
class Error(Exception):
    """æ­¤æ¨¡çµ„æ‹‹å‡ºçš„æ‰€æœ‰ä¾‹å¤–çš„åŸºç¤é¡åˆ¥"""
    pass

class InValidBeautifulSoupTag(Error):
    """å› ç‚ºç„¡æ•ˆçš„ BeautifulSoup æ¨™ç±¤è€Œç„¡æ³•å»ºç«‹ ArticleSummary"""
    pass

class NoGivenURLForPage(Error):
    """å»ºç«‹é é¢æ™‚çµ¦å®šäº† None æˆ–ç©ºç™½çš„ URL"""
    pass

class PageNotFound(Error):
    """ç„¡æ³•é€éçµ¦å®šçš„ URL å–å¾—é é¢"""
    pass

class ArtitcleIsRemoved(Error):
    """ç„¡æ³•å¾ ArticleSummary è®€å–å·²è¢«åˆªé™¤çš„æ–‡ç« """
    pass


# å·¥å…·å‡½æ•¸
def parse_std_url(url):
    """è§£ææ¨™æº–çš„ PTT URL"""
    prefix, _, basename = url.rpartition('/')
    basename, _, _ = basename.rpartition('.')
    bbs, _, board = prefix.rpartition('/')
    bbs = bbs[1:]
    return bbs, board, basename


def parse_title(title):
    """è§£ææ–‡ç« æ¨™é¡Œä»¥ç²å–æ›´å¤šè³‡è¨Š"""
    isreply = 'Re:' in title
    isforward = 'Fw:' in title
    
    start_bracket = title.find('[')
    if start_bracket == -1:
        return 'ç„¡åˆ†é¡', isreply, isforward
    
    end_bracket = title.find(']', start_bracket)
    if end_bracket == -1:
        return 'ç„¡åˆ†é¡', isreply, isforward
    
    category = title[start_bracket + 1:end_bracket].strip()
    
    if not category:
        return 'ç„¡åˆ†é¡', isreply, isforward
    
    return category, isreply, isforward


def parse_username(full_name):
    """è§£æç”¨æˆ¶åç¨±ä»¥ç²å–å…¶ç”¨æˆ¶å¸³è™Ÿå’Œæš±ç¨±"""
    if ' (' not in full_name:
        return full_name, ''
    name, nickname = full_name.split(' (', 1)
    nickname = nickname.rstrip(')')
    return name, nickname


# Msg æ˜¯ä¸€å€‹ namedtupleï¼Œç”¨æ–¼æ¨¡å‹åŒ–æ¨æ–‡çš„è³‡è¨Š
Msg = collections.namedtuple('Msg', ['type', 'user', 'content', 'ipdatetime'])


class ArticleSummary:
    """ç”¨æ–¼æ¨¡å‹åŒ–æ–‡ç« è³‡è¨Šçš„é¡åˆ¥ï¼Œè©²è³‡è¨Šä¾†è‡ª ArticleListPage"""

    def __init__(self, title, url, score, date, author, mark, removeinfo):
        # æ¨™é¡Œ
        self.title = title
        self.category, self.isreply, self.isforward = parse_title(title)

        # URL
        self.url = url
        _, self.board, self.aid = parse_std_url(url)

        # å…ƒè³‡æ–™
        self.score = score
        self.date = date
        self.author = author
        self.mark = mark

        # åˆªé™¤è³‡è¨Š
        self.isremoved = True if removeinfo else False
        self.removeinfo = removeinfo

    @classmethod
    def from_bs_tag(cls, tag):
        """å¾å°æ‡‰çš„ bs æ¨™ç±¤å»ºç«‹ ArticleSummary ç‰©ä»¶çš„é¡åˆ¥æ–¹æ³•"""
        try:
            removeinfo = None
            title_tag = tag.find('div', class_='title')
            a_tag = title_tag.find('a')

            if not a_tag:
                removeinfo = title_tag.get_text().strip()

            if not removeinfo:
                title = a_tag.get_text().strip()
                url = a_tag.get('href').strip()
                score = tag.find('div', class_='nrec').get_text().strip()
            else:
                title = 'æœ¬æ–‡ç« å·²è¢«åˆªé™¤'
                url = ''
                score = ''

            date = tag.find('div', class_='date').get_text().strip()
            author = tag.find('div', class_='author').get_text().strip()
            mark = tag.find('div', class_='mark').get_text().strip()
        except Exception:
            raise InValidBeautifulSoupTag(tag)

        return cls(title, url, score, date, author, mark, removeinfo)

    def __repr__(self):
        return '<Summary of Article("{}")>'.format(self.url)

    def __str__(self):
        return self.title

    def read(self):
        """å¾ URL è®€å–æ–‡ç« ä¸¦è¿”å› ArticlePage"""
        if self.isremoved:
            raise ArtitcleIsRemoved(self.removeinfo)
        return ArticlePage(self.url)


class Page:
    """é é¢çš„åŸºç¤é¡åˆ¥"""
    ptt_domain = 'https://www.ptt.cc'

    def __init__(self, url):
        if not url:
            raise NoGivenURLForPage

        self.url = url
        url = urllib.parse.urljoin(self.ptt_domain, self.url)
        
        # ä½¿ç”¨ fake-useragent å’Œ 1 ç§’è¶…æ™‚
        try:
            ua = UserAgent()
            user_agent = ua.random
        except:
            user_agent = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'
        
        resp = requests.get(
            url=url, 
            cookies={'over18': '1'}, 
            verify=True, 
            timeout=PTT_TIMEOUT,
            headers={'User-Agent': user_agent}
        )

        if resp.status_code == requests.codes.ok:
            self.html = resp.text
        else:
            raise PageNotFound(f"HTTP {resp.status_code}")


class ArticleListPage(Page):
    """ç”¨æ–¼æ¨¡å‹åŒ–æ–‡ç« åˆ—è¡¨é é¢çš„é¡åˆ¥"""

    def __init__(self, url):
        super().__init__(url)

        # è¨­å®šæ–‡ç« æ¨™ç±¤
        soup = BeautifulSoup(self.html, 'html.parser')
        self.article_summary_tags = soup.find_all('div', 'r-ent')
        self.article_summary_tags.reverse()

        # è¨­å®šç›¸é—œ URL
        action_tags = soup.find('div', class_='action-bar').find_all('a')
        self.related_urls = {}
        url_names = 'board man oldest previous next newest'
        for idx, name in enumerate(url_names.split()):
            self.related_urls[name] = action_tags[idx].get('href')

        # è¨­å®šç‰ˆé¢å’Œç´¢å¼•
        _, self.board, basename = parse_std_url(url)
        _, _, idx = basename.partition('index')
        if idx:
            self.idx = int(idx)
        else:
            _, self.board, basename = parse_std_url(self.related_urls['previous'])
            _, _, idx = basename.partition('index')
            self.idx = int(idx)+1

    @classmethod
    def from_board(cls, board, index=''):
        """å¾çµ¦å®šçš„ç‰ˆåå’Œç´¢å¼•å»ºç«‹ ArticleListPage ç‰©ä»¶çš„é¡åˆ¥æ–¹æ³•"""
        url = '/'.join(['/bbs', board, 'index'+str(index)+'.html'])
        return cls(url)

    def __repr__(self):
        return 'ArticleListPage("{}")'.format(self.url)

    def __iter__(self):
        return self.article_summaries

    def get_article_summary(self, index):
        return ArticleSummary.from_bs_tag(self.article_summary_tags[index])

    @property
    def article_summaries(self):
        return (ArticleSummary.from_bs_tag(tag) for tag in self.article_summary_tags)

    @property
    def previous(self):
        return ArticleListPage(self.related_urls['previous'])

    @property
    def next(self):
        return ArticleListPage(self.related_urls['next'])

    @property
    def oldest(self):
        return ArticleListPage(self.related_urls['oldest'])

    @property
    def newest(self):
        return ArticleListPage(self.related_urls['newest'])


class ArticlePage(Page):
    """ç”¨æ–¼æ¨¡å‹åŒ–æ–‡ç« é é¢çš„é¡åˆ¥"""

    def __init__(self, url):
        super().__init__(url)
        self.soup = BeautifulSoup(self.html, 'html.parser')

        # è¨­å®šåŸºæœ¬è³‡è¨Š
        _, self.board, self.aid = parse_std_url(url)
        self.url = url

        # è¨­å®šæ–‡ç« ä½œè€…ã€æ¨™é¡Œå’Œæ™‚é–“
        main_content = self.soup.find('div', id='main-content')
        metas = main_content.find_all('div', class_='article-metaline')

        try:
            self.author = metas[0].find('span', class_='article-meta-value').get_text()
            self.title = metas[1].find('span', class_='article-meta-value').get_text()
            self.datetime_str = metas[2].find('span', class_='article-meta-value').get_text()
            self.datetime = datetime.datetime.strptime(self.datetime_str, '%a %b %d %H:%M:%S %Y')
            self.date = self.datetime_str
        except (IndexError, ValueError):
            self.author = ''
            self.title = ''
            self.datetime_str = ''
            self.datetime = None
            self.date = ''

        # è§£ææ¨™é¡Œåˆ†é¡
        self.category, self.isreply, self.isforward = parse_title(self.title)

        # é‡è¦ï¼šå…ˆè¨­å®šæ¨æ–‡ï¼ˆåœ¨ç§»é™¤æ¨æ–‡æ¨™ç±¤ä¹‹å‰ï¼‰
        self.pushes = PushesHandler(self.soup)

        # è¨­å®šæ–‡ç« å…§å®¹ï¼ˆé€™æœƒç§»é™¤æ¨æ–‡æ¨™ç±¤ï¼‰
        self._set_content()

        # è¨­å®šæ–‡ç«  IP
        self._set_ip()

    def _set_content(self):
        """è¨­å®šæ–‡ç« å…§å®¹"""
        main_content = self.soup.find('div', id='main-content')
        
        # ç§»é™¤ metaline
        for meta in main_content.find_all('div', class_='article-metaline'):
            meta.extract()
        for meta in main_content.find_all('div', class_='article-metaline-right'):
            meta.extract()
        
        # ç§»é™¤æ¨æ–‡
        for push in main_content.find_all('div', class_='push'):
            push.extract()
            
        self.content = main_content.get_text().strip()

    def _set_ip(self):
        """è¨­å®šæ–‡ç«  IP"""
        try:
            ip_tag = self.soup.find('span', class_='f2')
            if ip_tag:
                ip_text = ip_tag.get_text()
                # å°‹æ‰¾ IP ä½å€æ ¼å¼
                import re
                ip_match = re.search(r'(\d+\.\d+\.\d+\.\d+)', ip_text)
                if ip_match:
                    self.ip = ip_match.group(1)
                else:
                    self.ip = ''
            else:
                self.ip = ''
        except:
            self.ip = ''

    def __repr__(self):
        return 'ArticlePage("{}")'.format(self.url)


class PushesHandler:
    """ç”¨æ–¼è™•ç†æ¨æ–‡çš„é¡åˆ¥"""

    def __init__(self, soup):
        self.soup = soup
        self.pushes = self._parse_pushes()
        self.count = self._count_pushes()
        self.simple_expression = self._simple_expression()

    def _parse_pushes(self):
        """è§£ææ¨æ–‡"""
        pushes = []
        push_tags = self.soup.find_all('div', class_='push')
        
        for push_tag in push_tags:
            try:
                push_type = push_tag.find('span', class_='push-tag').get_text().strip()
                push_user = push_tag.find('span', class_='push-userid').get_text().strip()
                push_content = push_tag.find('span', class_='push-content').get_text().strip()
                push_ipdatetime = push_tag.find('span', class_='push-ipdatetime').get_text().strip()
                
                pushes.append(Msg(push_type, push_user, push_content, push_ipdatetime))
            except:
                continue
                
        return pushes

    def _count_pushes(self):
        """è¨ˆç®—æ¨æ–‡æ•¸é‡"""
        count = {'all': 0, 'like': 0, 'boo': 0, 'neutral': 0}
        
        for push in self.pushes:
            count['all'] += 1
            if 'æ¨' in push.type:
                count['like'] += 1
            elif 'å™“' in push.type:
                count['boo'] += 1
            else:
                count['neutral'] += 1
                
        count['score'] = count['like'] - count['boo']
        return count

    def _simple_expression(self):
        """ç°¡åŒ–çš„æ¨æ–‡è¡¨é”"""
        return [f"{push.type} {push.user}: {push.content}" for push in self.pushes]


# å…¨åŸŸåªå»ºç«‹ä¸€æ¬¡ engineã€metadataã€table ä¸¦ create_all
address = f"mysql+pymysql://{MYSQL_ACCOUNT}:{MYSQL_PASSWORD}@{MYSQL_HOST}:{MYSQL_PORT}/{MYSQL_DATABASE}"
engine = create_engine(address)
metadata = MetaData()

# PTT æ–‡ç« è³‡æ–™è¡¨çµæ§‹
ptt_articles_table = Table(
    "ptt_articles",
    metadata,
    Column("aid", String(20), primary_key=True),  # æ–‡ç« ç·¨ç¢¼ä½œç‚ºä¸»éµ
    Column("board", String(50)),  # ç‰ˆå
    Column("author", String(100)),  # ä½œè€…
    Column("title", String(500)),  # æ¨™é¡Œ
    Column("category", String(100)),  # åˆ†é¡
    Column("content", Text),  # å…§æ–‡
    Column("date", String(100)),  # æ—¥æœŸï¼ˆåŸå§‹æ ¼å¼ï¼‰
    Column("ip", String(50)),  # IPä½ç½®
    Column("pushes_all", Integer),  # ç¸½ç•™è¨€æ•¸
    Column("pushes_like", Integer),  # æ¨
    Column("pushes_boo", Integer),  # å™“
    Column("pushes_neutral", Integer),  # ä¸­ç«‹
    Column("pushes_score", Integer),  # æ–‡ç« åˆ†æ•¸
    Column("url", String(200)),  # æ–‡ç«  URL
    Column("crawl_time", Date),  # çˆ¬å–æ™‚é–“
)

def init_database():
    """åˆå§‹åŒ–è³‡æ–™åº«ï¼Œå»ºç«‹è³‡æ–™è¡¨"""
    try:
        metadata.create_all(engine)
        print("PTT æ–‡ç« è³‡æ–™è¡¨å·²åˆå§‹åŒ–")
        return True
    except Exception as e:
        print(f"åˆå§‹åŒ–è³‡æ–™åº«å¤±æ•—: {e}")
        return False


def upload_ptt_data_to_mysql(df: pd.DataFrame):
    """å°‡ PTT æ–‡ç« è³‡æ–™ä¸Šå‚³åˆ° MySQL è³‡æ–™åº«"""
    if df.empty:
        print("ç„¡è³‡æ–™éœ€è¦ä¸Šå‚³")
        return 0

    print(f"æº–å‚™ä¸Šå‚³ {len(df)} ç­† PTT æ–‡ç« è³‡æ–™åˆ° MySQL...")

    # æº–å‚™è³‡æ–™
    df_copy = df.copy()
    df_copy['crawl_time'] = datetime.date.today()
    
    # ç¢ºä¿æ‰€æœ‰å¿…è¦æ¬„ä½å­˜åœ¨
    required_columns = {
        'aid': '',
        'board': PTT_BOARD,
        'author': '',
        'title': '',
        'category': 'ç„¡åˆ†é¡',
        'content': '',
        'date': '',
        'ip': '',
        'pushes_all': 0,
        'pushes_like': 0,
        'pushes_boo': 0,
        'pushes_neutral': 0,
        'pushes_score': 0,
        'url': ''
    }
    
    for col, default_value in required_columns.items():
        if col not in df_copy.columns:
            # å¦‚æœæ¬„ä½å®Œå…¨ä¸å­˜åœ¨ï¼Œæ‰æ·»åŠ ä¸¦è¨­ç‚ºé»˜èªå€¼
            df_copy[col] = default_value
        else:
            # å°æ–¼å­˜åœ¨çš„æ¬„ä½ï¼Œåªå¡«å……çœŸæ­£çš„ç©ºå€¼ï¼Œä½†ä¿ç•™æ•¸å€¼ 0
            if col.startswith('pushes_'):
                # å°æ–¼æ¨æ–‡ç›¸é—œæ¬„ä½ï¼Œåªå¡«å…… None å’Œ NaNï¼Œä¿ç•™æ•¸å€¼ 0
                df_copy[col] = df_copy[col].fillna(default_value)
                # ç¢ºä¿æ˜¯æ•´æ•¸é¡å‹
                df_copy[col] = df_copy[col].astype(int)
            else:
                # å°æ–¼å…¶ä»–æ¬„ä½ï¼Œæ­£å¸¸å¡«å……ç©ºå€¼
                df_copy[col] = df_copy[col].fillna(default_value)

    # åªä¿ç•™éœ€è¦çš„æ¬„ä½
    df_copy = df_copy[list(required_columns.keys()) + ['crawl_time']]
    
    # ===== DEBUG: è¼¸å‡ºè¦é€åˆ°è³‡æ–™åº«çš„è³‡æ–™å…§å®¹ =====
    print("=" * 80)
    print("ğŸ” DEBUG: æº–å‚™ä¸Šå‚³åˆ°è³‡æ–™åº«çš„è³‡æ–™:")
    print(f"ğŸ“Š è³‡æ–™ç­†æ•¸: {len(df_copy)}")
    print(f"ğŸ“‹ æ¬„ä½åç¨±: {list(df_copy.columns)}")
    print("ğŸ“ è³‡æ–™å…§å®¹:")
    for idx, row in df_copy.iterrows():
        print(f"  ç¬¬ {idx+1} ç­†:")
        for col, value in row.items():
            if col == 'content':
                # content å¯èƒ½å¾ˆé•·ï¼Œåªé¡¯ç¤ºå‰50å­—
                content_preview = str(value)[:50] + "..." if len(str(value)) > 50 else str(value)
                print(f"    {col}: {content_preview}")
            else:
                print(f"    {col}: {value}")
        print("-" * 40)
    print("=" * 80)
    # ===== END DEBUG =====

    try:
        # ä½¿ç”¨ MySQL çš„ ON DUPLICATE KEY UPDATE ä¾†è™•ç†é‡è¤‡è³‡æ–™
        with engine.connect() as conn:
            with conn.begin():  # ä½¿ç”¨äº‹å‹™
                data_dict = df_copy.to_dict('records')
                
                # ä½¿ç”¨ MySQL çš„ INSERT ... ON DUPLICATE KEY UPDATE
                stmt = insert(ptt_articles_table).values(data_dict)
                
                # å®šç¾©æ›´æ–°çš„æ¬„ä½ï¼ˆé™¤äº†ä¸»éµ aidï¼‰
                update_dict = {
                    col.name: stmt.inserted[col.name] 
                    for col in ptt_articles_table.columns 
                    if col.name != 'aid'
                }
                
                stmt = stmt.on_duplicate_key_update(**update_dict)
                result = conn.execute(stmt)
                
                print(f"æˆåŠŸè™•ç† {len(data_dict)} ç­† PTT æ–‡ç« è³‡æ–™ï¼ˆæ–°å¢æˆ–æ›´æ–°ï¼‰")
                return len(data_dict)
            
    except Exception as e:
        print(f"ä¸Šå‚³ PTT æ–‡ç« è³‡æ–™æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
        return 0


def ptt_crawl_single_page(board_name, page_index, target_date=None):
    """çˆ¬å–å–®ä¸€é é¢çš„æ–‡ç« è³‡æ–™"""
    print(f'æ­£åœ¨è™•ç† {board_name} ç‰ˆç¬¬ {page_index} é ')
    
    error_count = 0
    success_count = 0
    
    try:
        # æŠ“è©²æ¿é é¢çš„æ–‡ç« 
        latest_page = ArticleListPage.from_board(board_name, page_index)
    except Exception as e:
        print(f'ç„¡æ³•è¼‰å…¥é é¢ {page_index}ï¼ŒéŒ¯èª¤: {e}')
        return pd.DataFrame(), False

    # æº–å‚™è³‡æ–™æ”¶é›†çš„åˆ—è¡¨
    ptt_aid = []
    ptt_author = []
    ptt_board = []
    ptt_category = []
    ptt_title = []
    ptt_content = []
    ptt_url = []
    ptt_date = []
    ptt_ip = []
    ptt_all = []
    ptt_boo = []
    ptt_like = []
    ptt_neutral = []
    ptt_score = []
    ptt_comment = []

    should_stop = False
    old_articles_count = 0

    for summary in latest_page:
        if summary.isremoved:
            continue

        print(f'æ­£åœ¨æŠ“è³‡æ–™ä¸­...{summary.title[:50]}...')
        
        # éš¨æ©Ÿå»¶é²
        delay = random.uniform(PTT_DELAY_MIN, PTT_DELAY_MAX)
        time.sleep(delay)

        try:
            article = summary.read()
            
            # å¦‚æœæœ‰è¨­å®šç›®æ¨™æ—¥æœŸï¼Œæª¢æŸ¥æ–‡ç« æ—¥æœŸ
            if target_date and article.datetime:
                if article.datetime < target_date:
                    old_articles_count += 1
                    print(f'ğŸ“… æ–‡ç« æ—¥æœŸéèˆŠï¼š{article.datetime.strftime("%Y-%m-%d %H:%M")}ï¼Œè·³é')
                    if old_articles_count >= 10:
                        print(f'ğŸ“… ç™¼ç¾é€£çºŒ {old_articles_count} ç¯‡éèˆŠæ–‡ç« ï¼Œåœæ­¢çˆ¬å–æ­¤é ')
                        should_stop = True
                    continue
                else:
                    old_articles_count = 0
            
            # æ”¶é›†æ–‡ç« è³‡æ–™
            ptt_aid.append(article.aid)
            ptt_author.append(article.author)
            ptt_board.append(article.board)
            ptt_category.append(article.category)
            ptt_title.append(article.title)
            ptt_content.append(article.content)
            ptt_url.append(article.url)
            ptt_date.append(article.date)
            ptt_ip.append(article.ip)
            
            # å®‰å…¨åœ°æ”¶é›†æ¨æ–‡æ•¸æ“šï¼Œé¿å… NaN å€¼
            try:
                if hasattr(article, 'pushes') and article.pushes is not None:
                    # ç›´æ¥å˜—è©¦ç²å–æ¨æ–‡æ•¸æ“š
                    count_data = getattr(article.pushes, 'count', None)
                    if count_data is not None and isinstance(count_data, dict):
                        # æˆåŠŸç²å–æ¨æ–‡çµ±è¨ˆ
                        ptt_all.append(count_data.get('all', 0))
                        ptt_boo.append(count_data.get('boo', 0))
                        ptt_like.append(count_data.get('like', 0))
                        ptt_neutral.append(count_data.get('neutral', 0))
                        ptt_score.append(count_data.get('score', 0))
                        ptt_comment.append(getattr(article.pushes, 'simple_expression', []))
                        print(f"âœ… æ¨æ–‡æ•¸æ“š: ç¸½ {count_data.get('all', 0)}, æ¨ {count_data.get('like', 0)}, å™“ {count_data.get('boo', 0)}")
                    else:
                        # pushes å°è±¡å­˜åœ¨ä½† count ç„¡æ•ˆ
                        raise ValueError("pushes.count æ•¸æ“šç„¡æ•ˆ")
                else:
                    # article.pushes ä¸å­˜åœ¨æˆ–ç‚º None
                    raise ValueError("article.pushes ä¸å­˜åœ¨")
            except Exception as push_error:
                # å¦‚æœæ¨æ–‡æ•¸æ“šç²å–å¤±æ•—ï¼Œä½¿ç”¨é»˜èªå€¼
                ptt_all.append(0)
                ptt_boo.append(0)
                ptt_like.append(0)
                ptt_neutral.append(0)
                ptt_score.append(0)
                ptt_comment.append([])
                print(f"âš ï¸ æ¨æ–‡æ•¸æ“šç²å–å¤±æ•—ï¼Œä½¿ç”¨é»˜èªå€¼ 0: {str(push_error)}")

            success_count += 1

        except Exception as e:
            error_count += 1
            article_title = summary.title if hasattr(summary, 'title') and summary.title else 'unknown'
            print(f'è™•ç†æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {article_title[:30]}... - {str(e)[:100]}')
            
            # é‡è¦ï¼šå³ä½¿ç™¼ç”ŸéŒ¯èª¤ï¼Œä¹Ÿè¦æ·»åŠ å ä½æ•¸æ“šä»¥ä¿æŒåˆ—è¡¨é•·åº¦ä¸€è‡´
            # é€™äº›æ•¸æ“šæœƒåœ¨å¾ŒçºŒè¢«éæ¿¾æ‰
            ptt_aid.append('')  # ç©ºå­—ç¬¦ä¸²ï¼Œæœƒè¢«éæ¿¾
            ptt_author.append('')
            ptt_board.append(board_name)
            ptt_category.append('')
            ptt_title.append('')  # ç©ºæ¨™é¡Œï¼Œæœƒè¢«éæ¿¾æ‰
            ptt_content.append('')
            ptt_url.append('')
            ptt_date.append('')
            ptt_ip.append('')
            ptt_all.append(0)  # å ä½æ•¸æ“š
            ptt_boo.append(0)
            ptt_like.append(0)
            ptt_neutral.append(0)
            ptt_score.append(0)
            ptt_comment.append([])
            print(f"ğŸ“ æ·»åŠ å ä½æ•¸æ“šä»¥ä¿æŒåˆ—è¡¨ä¸€è‡´æ€§ï¼ˆå°‡è¢«éæ¿¾ï¼‰")
            
            continue

    # å»ºç«‹ DataFrameï¼ˆä½¿ç”¨è‹±æ–‡æ¬„ä½åç¨±ï¼Œå°æ‡‰è³‡æ–™åº«çµæ§‹ï¼‰
    print(f"\nğŸ“Š æº–å‚™å»ºç«‹ DataFrame:")
    print(f"  åˆ—è¡¨é•·åº¦æª¢æŸ¥:")
    print(f"    ptt_aid: {len(ptt_aid)}")
    print(f"    ptt_all: {len(ptt_all)}")
    print(f"    ptt_like: {len(ptt_like)}")
    print(f"    ptt_boo: {len(ptt_boo)}")
    print(f"    ptt_neutral: {len(ptt_neutral)}")
    
    dic = {
        'aid': ptt_aid,
        'author': ptt_author,
        'board': ptt_board,
        'category': ptt_category,
        'title': ptt_title,
        'content': ptt_content,
        'date': ptt_date,
        'ip': ptt_ip,
        'pushes_all': ptt_all,
        'pushes_boo': ptt_boo,
        'pushes_like': ptt_like,
        'pushes_neutral': ptt_neutral,
        'pushes_score': ptt_score,
        'url': ptt_url  # ä½¿ç”¨æ”¶é›†çš„ URL åˆ—è¡¨
    }
    
    final_data = pd.DataFrame(dic)
    print(f"ğŸ“‹ DataFrame å»ºç«‹å®Œæˆï¼ŒåŸå§‹æ•¸æ“š: {len(final_data)} ç­†")
    
    # é¡¯ç¤ºæ¨æ–‡æ•¸æ“šçµ±è¨ˆ
    print(f"ğŸ“ˆ æ¨æ–‡æ•¸æ“šçµ±è¨ˆ:")
    print(f"  æ¨æ–‡æ•¸ > 0 çš„æ–‡ç« : {len(final_data[final_data['pushes_all'] > 0])} ç­†")
    print(f"  æ¨æ–‡æ•¸ = 0 çš„æ–‡ç« : {len(final_data[final_data['pushes_all'] == 0])} ç­†")
    
    # éæ¿¾æ‰æ¨™é¡Œç‚ºç©ºçš„æ–‡ç« ï¼ˆéŒ¯èª¤è™•ç†ç”¢ç”Ÿçš„å ä½æ•¸æ“šï¼‰
    final_data = final_data[final_data['title'] != '']
    print(f"ğŸ“‹ éæ¿¾å¾Œæ•¸æ“š: {len(final_data)} ç­†ï¼ˆç§»é™¤äº† {len(dic['aid']) - len(final_data)} ç­†éŒ¯èª¤æ•¸æ“šï¼‰")

    print(f'é é¢è™•ç†å®Œæˆ - æˆåŠŸ: {success_count} ç­†ï¼ŒéŒ¯èª¤: {error_count} ç­†')
    
    if target_date:
        print(f'ğŸ“… éèˆŠæ–‡ç« : {old_articles_count} ç¯‡ï¼ˆæ—©æ–¼ {target_date.strftime("%Y-%m-%d")}ï¼‰')

    return final_data, should_stop


@app.task(bind=True)
def crawl_ptt_page_task(self, board_name=None, page_index=None, target_days=None):
    """Celery ä»»å‹™ï¼šçˆ¬å– PTT æŒ‡å®šé é¢"""
    # åˆå§‹åŒ–è³‡æ–™åº«
    if not init_database():
        return {'status': 'error', 'message': 'è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—'}
    
    if board_name is None:
        board_name = PTT_BOARD
    
    if page_index is None:
        # è‡ªå‹•åµæ¸¬æœ€æ–°é é¢
        try:
            index_url = f'https://www.ptt.cc/bbs/{board_name}/index.html'
            index_page = ArticleListPage(index_url)
            previous_url = index_page.previous.url
            page_index = int(previous_url[previous_url.find('index')+5:previous_url.find('.html')]) + 1
            print(f'è‡ªå‹•åµæ¸¬èµ·å§‹é é¢: {page_index}')
        except Exception as e:
            print(f'ç„¡æ³•å–å¾—èµ·å§‹é é¢ï¼Œä½¿ç”¨é è¨­å€¼: {e}')
            page_index = 1

    # è¨ˆç®—ç›®æ¨™æ—¥æœŸ
    target_date = None
    if target_days:
        target_date = datetime.datetime.now() - datetime.timedelta(days=target_days)
        print(f'ğŸ“… ç›®æ¨™æ—¥æœŸï¼š{target_date.strftime("%Yå¹´%mæœˆ%dæ—¥")} ä¹‹å¾Œçš„æ–‡ç« ')

    try:
        print(f"é–‹å§‹çˆ¬å– {board_name} ç‰ˆç¬¬ {page_index} é ")
        
        # çˆ¬å–å–®ä¸€é é¢
        df, should_stop = ptt_crawl_single_page(board_name, page_index, target_date)
        
        if not df.empty:
            # ä¸Šå‚³åˆ°è³‡æ–™åº«
            uploaded_count = upload_ptt_data_to_mysql(df)
            
            result = {
                'status': 'success',
                'board': board_name,
                'page': page_index,
                'articles_found': len(df),
                'articles_uploaded': uploaded_count,
                'should_stop': should_stop
            }
            
            print(f"âœ… ä»»å‹™å®Œæˆï¼š{board_name} ç¬¬ {page_index} é ï¼Œæ‰¾åˆ° {len(df)} ç¯‡æ–‡ç« ï¼Œä¸Šå‚³ {uploaded_count} ç­†")
            return result
            
        else:
            print(f"âš ï¸  ç¬¬ {page_index} é ç„¡æœ‰æ•ˆè³‡æ–™")
            return {
                'status': 'no_data',
                'board': board_name,
                'page': page_index,
                'articles_found': 0,
                'articles_uploaded': 0,
                'should_stop': should_stop
            }
            
    except Exception as e:
        print(f"âŒ çˆ¬å–ä»»å‹™å¤±æ•—ï¼š{str(e)}")
        return {
            'status': 'error',
            'board': board_name,
            'page': page_index,
            'error': str(e)
        }


@app.task(bind=True)
def crawl_ptt_recent_pages_task(self, board_name=None, target_days=7, max_pages=None):
    """Celery ä»»å‹™ï¼šçˆ¬å– PTT è¿‘æœŸæ–‡ç« ï¼ˆå¤šé ï¼‰"""
    # åˆå§‹åŒ–è³‡æ–™åº«
    if not init_database():
        return {'status': 'error', 'message': 'è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—'}
    
    if board_name is None:
        board_name = PTT_BOARD
    
    if max_pages is None:
        print(f"ğŸŒŸ é–‹å§‹çˆ¬å– {board_name} ç‰ˆè¿‘ {target_days} å¤©çš„æ–‡ç« ï¼ˆç„¡é æ•¸é™åˆ¶ï¼Œç›´åˆ°æ‰¾åˆ°æ‰€æœ‰æŒ‡å®šå¤©æ•¸å…§çš„æ–‡ç« ï¼‰")
    else:
        print(f"ğŸŒŸ é–‹å§‹çˆ¬å– {board_name} ç‰ˆè¿‘ {target_days} å¤©çš„æ–‡ç« ï¼ˆæœ€å¤š {max_pages} é ï¼‰")
    
    # è¨ˆç®—ç›®æ¨™æ—¥æœŸ
    target_date = datetime.datetime.now() - datetime.timedelta(days=target_days)
    print(f'ğŸ“… ç›®æ¨™æ—¥æœŸï¼š{target_date.strftime("%Yå¹´%mæœˆ%dæ—¥")} ä¹‹å¾Œçš„æ–‡ç« ')
    
    # å–å¾—èµ·å§‹é é¢
    try:
        index_url = f'https://www.ptt.cc/bbs/{board_name}/index.html'
        index_page = ArticleListPage(index_url)
        previous_url = index_page.previous.url
        start_page = int(previous_url[previous_url.find('index')+5:previous_url.find('.html')]) + 1
        print(f'è‡ªå‹•åµæ¸¬èµ·å§‹é é¢: {start_page}')
    except Exception as e:
        print(f'ç„¡æ³•å–å¾—èµ·å§‹é é¢ï¼Œä½¿ç”¨é è¨­å€¼: {e}')
        start_page = 1
    
    total_articles = 0
    total_uploaded = 0
    pages_processed = 0
    
    try:
        page_count = 0
        while True:
            current_page = start_page - page_count
            
            if current_page <= 0:
                print('å·²åˆ°é”æœ€æ—©é é¢ï¼Œçˆ¬å–å®Œæˆ')
                break
            
            # å¦‚æœè¨­å®šäº†æœ€å¤§é æ•¸é™åˆ¶ï¼Œæª¢æŸ¥æ˜¯å¦è¶…é
            if max_pages is not None and page_count >= max_pages:
                print(f'å·²é”åˆ°æœ€å¤§é æ•¸é™åˆ¶ ({max_pages} é )ï¼Œåœæ­¢çˆ¬å–')
                break
                
            print(f"\n--- è™•ç†ç¬¬ {page_count+1} é  (é é¢ç·¨è™Ÿ: {current_page}) ---")
            
            # çˆ¬å–å–®ä¸€é é¢
            df, should_stop = ptt_crawl_single_page(board_name, current_page, target_date)
            
            if not df.empty:
                # ä¸Šå‚³åˆ°è³‡æ–™åº«
                uploaded_count = upload_ptt_data_to_mysql(df)
                total_articles += len(df)
                total_uploaded += uploaded_count
                print(f'ç¬¬ {page_count+1} é å®Œæˆï¼ŒæˆåŠŸå–å¾— {len(df)} ç­†è³‡æ–™ï¼Œä¸Šå‚³ {uploaded_count} ç­†')
            else:
                print(f'ç¬¬ {page_count+1} é ç„¡æœ‰æ•ˆè³‡æ–™')
            
            pages_processed += 1
            page_count += 1
            
            # å¦‚æœç™¼ç¾éèˆŠæ–‡ç« ï¼Œåœæ­¢çˆ¬å–
            if should_stop:
                print(f'ğŸ“… ç™¼ç¾éèˆŠæ–‡ç« ï¼Œåœæ­¢çˆ¬å–')
                break
        
        result = {
            'status': 'success',
            'board': board_name,
            'target_days': target_days,
            'pages_processed': pages_processed,
            'total_articles': total_articles,
            'total_uploaded': total_uploaded
        }
        
        print(f"âœ… æ‰¹é‡çˆ¬å–å®Œæˆï¼š{board_name} ç‰ˆï¼Œè™•ç† {pages_processed} é ï¼Œæ‰¾åˆ° {total_articles} ç¯‡æ–‡ç« ï¼Œä¸Šå‚³ {total_uploaded} ç­†")
        return result
        
    except Exception as e:
        print(f"âŒ æ‰¹é‡çˆ¬å–ä»»å‹™å¤±æ•—ï¼š{str(e)}")
        return {
            'status': 'error',
            'board': board_name,
            'pages_processed': pages_processed,
            'total_articles': total_articles,
            'total_uploaded': total_uploaded,
            'error': str(e)
        }





def get_ptt_user_agent():
    """å–å¾—éš¨æ©Ÿ User-Agent"""
    try:
        from fake_useragent import UserAgent
        ua = UserAgent()
        return ua.random
    except:
        return 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'


def simple_ptt_crawl(board_name, page_index, target_date=None):
    """ç°¡åŒ–ç‰ˆ PTT çˆ¬èŸ²ï¼Œçˆ¬å–å–®ä¸€é é¢"""
    try:
        # å»ºæ§‹é é¢ URL
        if page_index:
            url = f'https://www.ptt.cc/bbs/{board_name}/index{page_index}.html'
        else:
            url = f'https://www.ptt.cc/bbs/{board_name}/index.html'
        
        print(f"æ­£åœ¨çˆ¬å–: {url}")
        
        # ç™¼é€è«‹æ±‚
        headers = {'User-Agent': get_ptt_user_agent()}
        response = requests.get(url, cookies={'over18': '1'}, headers=headers, timeout=PTT_TIMEOUT)
        
        if response.status_code != 200:
            print(f"é é¢è«‹æ±‚å¤±æ•—: {response.status_code}")
            return pd.DataFrame(), False
        
        # è§£æé é¢
        soup = BeautifulSoup(response.text, 'html.parser')
        article_tags = soup.find_all('div', 'r-ent')
        
        # å­˜å„²æ–‡ç« è³‡æ–™
        articles_data = []
        should_stop = False
        
        for tag in article_tags:
            try:
                # å–å¾—æ¨™é¡Œå’Œ URL
                title_tag = tag.find('div', class_='title')
                a_tag = title_tag.find('a') if title_tag else None
                
                if not a_tag:
                    continue  # è·³éå·²åˆªé™¤çš„æ–‡ç« 
                
                title = a_tag.get_text().strip()
                article_url = a_tag.get('href').strip()
                
                # å–å¾—å…¶ä»–è³‡è¨Š
                score_tag = tag.find('div', class_='nrec')
                score = score_tag.get_text().strip() if score_tag else ''
                
                date_tag = tag.find('div', class_='date')
                date = date_tag.get_text().strip() if date_tag else ''
                
                author_tag = tag.find('div', class_='author')
                author = author_tag.get_text().strip() if author_tag else ''
                
                # è§£ææ¨™é¡Œåˆ†é¡
                category, isreply, isforward = parse_title(title)
                
                # å»ºç«‹æ–‡ç« è³‡æ–™
                article_data = {
                    'aid': article_url.split('/')[-1].replace('.html', '') if article_url else '',
                    'board': board_name,
                    'author': author,
                    'title': title,
                    'category': category,
                    'content': '',  # ç°¡åŒ–ç‰ˆä¸çˆ¬å–å…§æ–‡
                    'date': date,
                    'ip': 'Unknown',  # ç°¡åŒ–ç‰ˆä¸çˆ¬å– IP
                    'pushes_all': 0,
                    'pushes_like': 0,
                    'pushes_boo': 0,
                    'pushes_neutral': 0,
                    'pushes_score': 0,
                    'url': article_url,
                }
                
                articles_data.append(article_data)
                
            except Exception as e:
                print(f"è™•ç†æ–‡ç« æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
                continue
        
        # è½‰æ›ç‚º DataFrame
        df = pd.DataFrame(articles_data)
        print(f"æˆåŠŸçˆ¬å– {len(df)} ç¯‡æ–‡ç« æ‘˜è¦")
        
        return df, should_stop
        
    except Exception as e:
        print(f"çˆ¬å–é é¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return pd.DataFrame(), False


@app.task()
def crawl_ptt_page(board_name=None, page_index='', target_days=3):
    """çˆ¬å– PTT ç‰ˆé¢æŒ‡å®šé é¢çš„æ–‡ç« æ‘˜è¦"""
    if not board_name:
        board_name = PTT_BOARD
    
    print(f"é–‹å§‹çˆ¬å– PTT {board_name} ç‰ˆï¼Œé é¢: {page_index if page_index else 'æœ€æ–°'}")
    
    try:
        # è¨ˆç®—ç›®æ¨™æ—¥æœŸ
        target_date = datetime.datetime.now() - datetime.timedelta(days=target_days)
        
        # çˆ¬å–é é¢
        df, should_stop = simple_ptt_crawl(board_name, page_index, target_date)
        
        if not df.empty:
            # ä¸Šå‚³åˆ° MySQL
            upload_ptt_data_to_mysql(df)
            print(f"PTT {board_name} ç‰ˆç¬¬ {page_index} é è³‡æ–™å·²æˆåŠŸä¸Šå‚³åˆ°è³‡æ–™åº«")
            return f"æˆåŠŸçˆ¬å–ä¸¦å„²å­˜ {len(df)} ç¯‡æ–‡ç« "
        else:
            print(f"PTT {board_name} ç‰ˆç¬¬ {page_index} é ç„¡æœ‰æ•ˆè³‡æ–™")
            return "ç„¡æœ‰æ•ˆè³‡æ–™"
            
    except Exception as e:
        error_msg = f"çˆ¬å– PTT {board_name} ç‰ˆç¬¬ {page_index} é æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
        print(error_msg)
        return error_msg


@app.task()
def crawl_ptt_recent(board_name=None, max_pages=10, target_days=3):
    """çˆ¬å– PTT ç‰ˆé¢è¿‘æœŸå¤šå€‹é é¢"""
    if not board_name:
        board_name = PTT_BOARD
    
    print(f"é–‹å§‹çˆ¬å– PTT {board_name} ç‰ˆè¿‘ {target_days} å¤©çš„æ–‡ç« ï¼Œæœ€å¤š {max_pages} é ")
    
    try:
        # å–å¾—èµ·å§‹é é¢ç·¨è™Ÿ
        index_url = f'https://www.ptt.cc/bbs/{board_name}/index.html'
        response = requests.get(index_url, cookies={'over18': '1'}, 
                              headers={'User-Agent': get_ptt_user_agent()}, timeout=PTT_TIMEOUT)
        
        if response.status_code != 200:
            return f"ç„¡æ³•å–å¾— {board_name} ç‰ˆé¦–é "
        
        soup = BeautifulSoup(response.text, 'html.parser')
        prev_link = soup.find('a', string='â€¹ ä¸Šé ')
        if prev_link:
            prev_url = prev_link.get('href')
            start_index = int(prev_url.split('index')[1].split('.html')[0]) + 1
        else:
            start_index = 1
        
        total_articles = 0
        
        # çˆ¬å–å¤šå€‹é é¢
        for page_offset in range(max_pages):
            page_index = start_index - page_offset
            if page_index <= 0:
                break
            
            print(f"çˆ¬å–ç¬¬ {page_offset + 1}/{max_pages} é  (index: {page_index})")
            
            df, should_stop = simple_ptt_crawl(board_name, page_index)
            
            if not df.empty:
                upload_ptt_data_to_mysql(df)
                total_articles += len(df)
                print(f"ç¬¬ {page_offset + 1} é å®Œæˆï¼Œç´¯è¨ˆ {total_articles} ç¯‡æ–‡ç« ")
            
            # åŠ å…¥å»¶é²
            delay = random.uniform(PTT_DELAY_MIN, PTT_DELAY_MAX)
            time.sleep(delay)
        
        return f"æˆåŠŸçˆ¬å–ä¸¦å„²å­˜ {total_articles} ç¯‡æ–‡ç« "
        
    except Exception as e:
        error_msg = f"çˆ¬å– PTT {board_name} ç‰ˆè¿‘æœŸé é¢æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}"
        print(error_msg)
        return error_msg


@app.task
def crawl_single_article_task(article_url):
    """
    å–®ç¯‡æ–‡ç« çˆ¬èŸ²ä»»å‹™
    æ¥æ”¶æ–‡ç« ç¶²å€ï¼Œçˆ¬å–è©²æ–‡ç« å…§å®¹ä¸¦å­˜å…¥è³‡æ–™åº«
    é¡ä¼¼ crawler_demo ä¸­çš„ crawler_finmind_duplicate ä»»å‹™
    """
    print(f"ğŸ”— é–‹å§‹çˆ¬å–å–®ç¯‡æ–‡ç« : {article_url}")
    
    try:
        # æª¢æŸ¥ URL æ ¼å¼
        if not article_url or not article_url.startswith('https://www.ptt.cc/bbs/'):
            print(f"âŒ ç„¡æ•ˆçš„æ–‡ç« ç¶²å€: {article_url}")
            return {"status": "error", "message": "ç„¡æ•ˆçš„æ–‡ç« ç¶²å€"}
        
        # çˆ¬å–æ–‡ç« å…§å®¹
        article_data = crawl_single_article(article_url)
        
        if not article_data:
            print(f"âš ï¸ ç„¡æ³•çˆ¬å–æ–‡ç« å…§å®¹: {article_url}")
            return {"status": "warning", "message": "ç„¡æ³•çˆ¬å–æ–‡ç« å…§å®¹"}
        
        # å„²å­˜åˆ°è³‡æ–™åº«
        df = pd.DataFrame([article_data])
        save_count = upload_ptt_data_to_mysql(df)
        
        print(f"âœ… æ–‡ç« çˆ¬å–å®Œæˆ: {article_data.get('title', 'unknown')[:30]}...")
        print(f"ğŸ’¾ è³‡æ–™åº«å„²å­˜: {save_count} ç­†")
        
        return {
            "status": "success",
            "article_url": article_url,
            "article_title": article_data.get('title', ''),
            "save_count": save_count,
            "message": "æ–‡ç« çˆ¬å–èˆ‡å„²å­˜æˆåŠŸ"
        }
        
    except Exception as e:
        error_msg = f"çˆ¬å–æ–‡ç« å¤±æ•—: {str(e)}"
        print(f"âŒ {error_msg}")
        
        return {
            "status": "error",
            "article_url": article_url,
            "error": error_msg
        }


def crawl_single_article(article_url):
    """
    çˆ¬å–å–®ç¯‡ PTT æ–‡ç« çš„è©³ç´°å…§å®¹
    """
    try:
        ua = UserAgent()
        headers = {'User-Agent': ua.random}
        
        response = requests.get(article_url, cookies={'over18': '1'}, 
                              headers=headers, timeout=PTT_TIMEOUT)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # å–å¾—æ–‡ç« åŸºæœ¬è³‡è¨Š
        main_content = soup.find('div', id='main-content')
        if not main_content:
            print(f"âš ï¸ æ‰¾ä¸åˆ°æ–‡ç« å…§å®¹: {article_url}")
            return None
        
        # è§£æ meta è³‡è¨Š
        metas = main_content.find_all('div', class_='article-metaline')
        author = ""
        title = ""
        date = ""
        
        try:
            if len(metas) >= 3:
                author = metas[0].find('span', class_='article-meta-value').get_text().strip()
                title = metas[1].find('span', class_='article-meta-value').get_text().strip()
                date = metas[2].find('span', class_='article-meta-value').get_text().strip()
        except:
            # ä½¿ç”¨å‚™ç”¨æ–¹æ³•
            title_element = soup.find('meta', property='og:title')
            title = title_element['content'] if title_element else "ç„¡æ¨™é¡Œ"
        
        # è§£ææ¨™é¡Œåˆ†é¡
        category, isreply, isforward = parse_title(title)
        
        # å–å¾—æ–‡ç«  ID
        aid = article_url.split('/')[-1].replace('.html', '') if article_url else ''
        
        # å–å¾—ç‰ˆé¢åç¨±
        board = extract_board_from_url(article_url)
        
        # ç§»é™¤ meta æ¨™ç±¤ä¸¦å–å¾—å…§æ–‡
        content_copy = main_content.__copy__()
        for meta in content_copy.find_all('div', class_='article-metaline'):
            meta.extract()
        for meta in content_copy.find_all('div', class_='article-metaline-right'):
            meta.extract()
        
        # ç§»é™¤æ¨æ–‡
        for push in content_copy.find_all('div', class_='push'):
            push.extract()
            
        content = content_copy.get_text().strip()
        
        # å–å¾— IP
        ip = ""
        try:
            ip_tag = soup.find('span', class_='f2')
            if ip_tag:
                ip_text = ip_tag.get_text()
                import re
                ip_match = re.search(r'(\d+\.\d+\.\d+\.\d+)', ip_text)
                if ip_match:
                    ip = ip_match.group(1)
        except:
            pass
        
        # è§£ææ¨æ–‡
        push_tags = soup.find_all('div', class_='push')
        pushes_all = len(push_tags)
        pushes_like = 0
        pushes_boo = 0
        pushes_neutral = 0
        
        for push_tag in push_tags:
            try:
                push_type = push_tag.find('span', class_='push-tag').get_text().strip()
                if 'æ¨' in push_type:
                    pushes_like += 1
                elif 'å™“' in push_type:
                    pushes_boo += 1
                else:
                    pushes_neutral += 1
            except:
                pushes_neutral += 1
        
        pushes_score = pushes_like - pushes_boo
        
        # å»ºç«‹æ–‡ç« è³‡æ–™ (ç¬¦åˆè³‡æ–™åº«çµæ§‹)
        article_data = {
            'aid': aid,
            'board': board,
            'author': author,
            'title': title,
            'category': category,
            'content': content,
            'date': date,
            'ip': ip,
            'pushes_all': pushes_all,
            'pushes_like': pushes_like,
            'pushes_boo': pushes_boo,
            'pushes_neutral': pushes_neutral,
            'pushes_score': pushes_score,
            'url': article_url
        }
        
        return article_data
        
    except Exception as e:
        print(f"âŒ çˆ¬å–æ–‡ç« å¤±æ•— {article_url}: {e}")
        return None


def extract_board_from_url(url):
    """å¾æ–‡ç« ç¶²å€ä¸­æå–ç‰ˆé¢åç¨±"""
    try:
        # URL æ ¼å¼: https://www.ptt.cc/bbs/Drink/M.1234567890.A.html
        parts = url.split('/')
        if len(parts) >= 5:
            return parts[4]  # å–å¾—ç‰ˆé¢åç¨±
    except:
        pass
    return "unknown"
