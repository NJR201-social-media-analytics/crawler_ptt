"""
PTT çˆ¬èŸ²ä»»å‹™ç™¼é€å™¨ - åˆ†æ•£å¼æ¶æ§‹
ç™¼é€ä»»å‹™åˆ° Celery ä½‡åˆ—ï¼Œæ¡ç”¨åˆ†æ•£å¼è™•ç†æ¨¡å¼
"""
import datetime
import time
import requests
from bs4 import BeautifulSoup
from fake_useragent import UserAgent


def send_distributed_crawl_task(board_name='Drink', target_days=30, max_pages=None):
    """
    ç™¼é€åˆ†æ•£å¼ PTT çˆ¬èŸ²ä»»å‹™
    æ¡ç”¨ã€ŒProducer åˆ†æé é¢ï¼Œç›´æ¥åˆ†ç™¼æ–‡ç« ä»»å‹™çµ¦ Worker ä¸¦è¡Œçˆ¬å–ã€çš„æµç¨‹

    Args:
        board_name: ç‰ˆé¢åç¨± (å¦‚ 'Drink')
        target_days: çˆ¬å–æœ€è¿‘å¹¾å¤©çš„æ–‡ç« 
        max_pages: æœ€å¤šè™•ç†å¹¾é  (None = ç„¡é™åˆ¶)

    åˆ†æ•£å¼è™•ç†æµç¨‹:
    1. Worker å•Ÿå‹•æ™‚è‡ªå‹•å®Œæˆè³‡æ–™è¡¨åˆå§‹åŒ–ï¼ˆé¿å…å¤šç¨‹åºç«¶çˆ­ï¼‰
    2. Producer é€é åˆ†æç‰ˆé¢ï¼Œæ”¶é›†æ‰€æœ‰ç¬¦åˆæ¢ä»¶çš„æ–‡ç« URL
    3. Producer å°‡æ–‡ç« ä»»å‹™åˆ†ç™¼çµ¦ Worker æ± ä¸¦è¡Œè™•ç†
    4. Worker è™•ç†å–®ç¯‡æ–‡ç« çˆ¬å–èˆ‡è³‡æ–™åº«å„²å­˜
    """
    from crawler.tasks_ptt_crawler import crawl_single_article_task
    import random

    print(f"ğŸš€ é–‹å§‹åˆ†æ•£å¼ PTT çˆ¬èŸ²ä»»å‹™")
    print(f"ğŸ“ ç›®æ¨™ç‰ˆé¢ï¼š{board_name}")
    print(f"ğŸ“… çˆ¬å–ç¯„åœï¼šæœ€è¿‘ {target_days} å¤©çš„æ–‡ç« ")
    print(f"ğŸ“„ æœ€å¤§é æ•¸ï¼š{max_pages if max_pages else 'ç„¡é™åˆ¶'}")

    # ç§»é™¤ Producer ç«¯åˆå§‹åŒ–ï¼šWorker è‡ªå‹•è™•ç†è³‡æ–™è¡¨åˆå§‹åŒ–
    print(f"ğŸ› ï¸ è³‡æ–™è¡¨åˆå§‹åŒ–å·²äº¤ç”± Worker è‡ªå‹•è™•ç†")

    target_date = datetime.datetime.now() - datetime.timedelta(days=target_days)
    print(f"ğŸ“… ç›®æ¨™æ—¥æœŸï¼š{target_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} ä¹‹å¾Œçš„æ–‡ç« ")

    total_articles_collected = 0
    total_tasks_sent = 0
    all_article_urls = []  # æ”¶é›†æ‰€æœ‰æ–‡ç« URL

    try:
        # ç¬¬ä¸€éšæ®µï¼šProducer è‡ªå·±åˆ†æé é¢ï¼Œæ”¶é›†æ‰€æœ‰æ–‡ç« URL
        print(f"\nğŸ” ç¬¬ä¸€éšæ®µï¼šåˆ†æé é¢ä¸¦æ”¶é›†æ–‡ç« URL")
        print(f"ğŸŒ æ­£åœ¨å–å¾— {board_name} ç‰ˆçš„èµ·å§‹é é¢...")
        
        base_url = f"https://www.ptt.cc/bbs/{board_name}/index.html"
        ua = UserAgent()
        headers = {'User-Agent': ua.random}

        response = requests.get(base_url, cookies={'over18': '1'},
                                headers=headers, timeout=10)
        response.raise_for_status()

        soup = BeautifulSoup(response.text, 'html.parser')

        # æ‰¾åˆ°ä¸Šä¸€é çš„é€£çµä¾†æ¨ç®—ç•¶å‰é ç¢¼
        prev_link = soup.find('a', string='â€¹ ä¸Šé ')
        if prev_link and prev_link.get('href'):
            import re
            match = re.search(r'index(\d+)\.html', prev_link['href'])
            if match:
                current_page = int(match.group(1)) + 1
            else:
                current_page = 1
        else:
            current_page = 1

        print(f"ğŸ“„ æ‰¾åˆ°æœ€æ–°é é¢ç·¨è™Ÿ: {current_page}")

        # é€é åˆ†æï¼Œæ”¶é›†æ–‡ç« URL
        pages_processed = 0
        max_pages_to_check = max_pages if max_pages else 50
        
        for page_offset in range(max_pages_to_check):
            current_page_num = current_page - page_offset
            if current_page_num <= 0:
                break

            page_url = f"https://www.ptt.cc/bbs/{board_name}/index{current_page_num}.html"
            print(f"\n--- ï¿½ åˆ†æç¬¬ {current_page_num} é  ---")
            
            # Producer è‡ªå·±åˆ†æé é¢
            page_articles = analyze_page_for_articles(page_url, target_days, current_page_num)
            
            if page_articles is None:  # ç™¼ç”ŸéŒ¯èª¤
                continue
                
            if page_articles['should_stop']:
                print(f"ğŸ›‘ ç¬¬ {current_page_num} é ç™¼ç¾éèˆŠæ–‡ç« ï¼Œåœæ­¢åˆ†æ")
                break
                
            # æ”¶é›†é€™ä¸€é çš„æ–‡ç« URL
            page_article_urls = page_articles['article_urls']
            all_article_urls.extend(page_article_urls)
            
            print(f"ğŸ“Š ç¬¬ {current_page_num} é çµæœ:")
            print(f"   ğŸ“° ç¸½æ–‡ç« æ•¸: {page_articles['total_articles']}")
            print(f"   âœ… ç¬¦åˆæ¢ä»¶: {len(page_article_urls)} ç¯‡")
            print(f"   ğŸ“Œ è·³éç½®é ‚: {page_articles['skipped_pinned']} ç¯‡")
            print(f"   â° éèˆŠæ–‡ç« : {page_articles['old_articles']} ç¯‡")
            
            total_articles_collected += len(page_article_urls)
            pages_processed += 1
            
            # çŸ­æš«å»¶é²
            time.sleep(random.uniform(1, 2))

        print(f"\nğŸ“Š é é¢åˆ†æå®Œæˆï¼")
        print(f"   ï¿½ åˆ†æé é¢æ•¸: {pages_processed}")
        print(f"   ğŸ“° æ”¶é›†æ–‡ç« æ•¸: {len(all_article_urls)}")

        # ç¬¬äºŒéšæ®µï¼šåˆ†ç™¼æ‰€æœ‰æ–‡ç« ä»»å‹™
        print(f"\nï¿½ ç¬¬äºŒéšæ®µï¼šåˆ†ç™¼æ–‡ç« çˆ¬å–ä»»å‹™")
        article_tasks = []
        
        for i, article_url in enumerate(all_article_urls, 1):
            print(f"ğŸ“¤ åˆ†ç™¼æ–‡ç« ä»»å‹™ {i}/{len(all_article_urls)}: {article_url.split('/')[-1]}")
            task = crawl_single_article_task.apply_async(
                args=[article_url],
                queue='ptt'
            )
            article_tasks.append({
                'task': task,
                'url': article_url,
                'task_id': task.id
            })
            total_tasks_sent += 1

        print(f"\nğŸ“‹ å·²åˆ†ç™¼ {len(article_tasks)} å€‹æ–‡ç« çˆ¬å–ä»»å‹™")

        # ç¬¬ä¸‰éšæ®µï¼šFire-and-forget æ¨¡å¼ï¼Œä¸ç­‰å¾…ä»»å‹™çµæœ
        print(f"\nğŸ¯ ç¬¬ä¸‰éšæ®µï¼šä»»å‹™å·²åˆ†ç™¼ï¼Œæ¡ç”¨ fire-and-forget æ¨¡å¼")
        print(f"ğŸ“‹ åˆ†ç™¼çš„ä»»å‹™ ID åˆ—è¡¨ï¼š")
        for task_info in article_tasks:
            print(f"   ğŸ“ {task_info['task_id'][:16]}... -> {task_info['url'].split('/')[-1]}")

        print(f"\nğŸ¯ åˆ†æ•£å¼çˆ¬èŸ²ä»»å‹™åˆ†ç™¼å®Œæˆ!")
        print(f"ğŸ“Š æœ€çµ‚çµ±è¨ˆ:")
        print(f"   ğŸ“„ åˆ†æé é¢æ•¸: {pages_processed}")
        print(f"   ğŸ“° æ”¶é›†æ–‡ç« æ•¸: {len(all_article_urls)}")
        print(f"   ğŸ”— åˆ†ç™¼ä»»å‹™æ•¸: {total_tasks_sent}")
        print(f"   ğŸš€ ä»»å‹™æ¨¡å¼: Fire-and-forget (ä¸ç­‰å¾…çµæœ)")
        print(f"\nğŸ’¡ ç›£æ§æ–¹å¼:")
        print(f"   ğŸŒ¸ Flower ç›£æ§ä»‹é¢: http://localhost:5555")
        print(f"   ğŸ“‹ Worker æ—¥èªŒ: make logs")
        print(f"   ğŸ—„ï¸ phpMyAdmin è³‡æ–™åº«: http://localhost:8000")
        print(f"\nâš ï¸  æ³¨æ„ï¼šWorker æœƒåœ¨èƒŒæ™¯è™•ç†é€™äº›ä»»å‹™ï¼Œè«‹é€éä¸Šè¿°ç›£æ§æ–¹å¼æŸ¥çœ‹é€²åº¦ã€‚")

        return {
            'status': 'success',
            'pages_processed': pages_processed,
            'articles_collected': len(all_article_urls),
            'tasks_sent': total_tasks_sent,
            'mode': 'fire-and-forget'
        }

    except Exception as e:
        print(f"âŒ åˆ†æ•£å¼çˆ¬èŸ²ä»»å‹™ç™¼é€å¤±æ•—: {e}")
        return {'status': 'error', 'message': str(e)}


def analyze_page_for_articles(page_url, target_days, page_number):
    """
    Producer è‡ªå·±åˆ†æé é¢ï¼Œæ”¶é›†æ–‡ç« URL
    
    Args:
        page_url: é é¢ç¶²å€
        target_days: ç›®æ¨™å¤©æ•¸
        page_number: é é¢ç·¨è™Ÿ
        
    Returns:
        dict: åŒ…å«æ–‡ç« URLåˆ—è¡¨å’Œçµ±è¨ˆè³‡è¨Š
    """
    import random
    from crawler.config import PTT_DELAY_MIN, PTT_DELAY_MAX, PTT_TIMEOUT
    
    try:
        ua = UserAgent()
        headers = {'User-Agent': ua.random}
        
        time.sleep(random.uniform(PTT_DELAY_MIN, PTT_DELAY_MAX))
        response = requests.get(page_url, cookies={'over18': '1'},
                               headers=headers, timeout=PTT_TIMEOUT)
        response.raise_for_status()
        
        soup = BeautifulSoup(response.text, 'html.parser')
        
        # ä½¿ç”¨ CSS é¸æ“‡å™¨å–å¾—æ‰€æœ‰æ–‡ç« 
        all_articles = soup.select('div.r-ent')
        if not all_articles:
            print("âŒ æ‰¾ä¸åˆ°ä»»ä½•æ–‡ç« ")
            return None
            
        # åªæœ‰ç¬¬ä¸€é ï¼ˆæœ€æ–°é é¢ï¼‰éœ€è¦æª¢æŸ¥åˆ†éš”ç·š
        articles_to_process = []
        skipped_pinned = 0
        
        # åˆ¤æ–·æ˜¯å¦ç‚ºç¬¬ä¸€é ï¼ˆé€šéæª¢æŸ¥æ˜¯å¦æœ‰åˆ†éš”ç·šï¼‰
        separator = soup.select_one('div.r-list-sep')
        
        if separator:
            # ç¬¬ä¸€é ï¼šåªè™•ç†åˆ†éš”ç·šä¹‹å‰çš„æ–‡ç« 
            print(f"ğŸ“Œ ç¬¬ä¸€é ï¼šç™¼ç¾åˆ†éš”ç·šï¼Œå°‡æ’é™¤ç½®é ‚æ–‡ç« ")
            list_container = soup.select_one('div.r-list-container')
            if list_container:
                all_elements = list_container.select('div.r-ent, div.r-list-sep')
                for element in all_elements:
                    if 'r-list-sep' in element.get('class', []):
                        # é‡åˆ°åˆ†éš”ç·šï¼Œåœæ­¢æ”¶é›†æ–‡ç« 
                        print(f"ğŸ“Œ åˆ†éš”ç·šå‰å…±æ”¶é›† {len(articles_to_process)} ç¯‡æ–‡ç« ")
                        break
                    elif 'r-ent' in element.get('class', []):
                        articles_to_process.append(element)
                skipped_pinned = len(all_articles) - len(articles_to_process)
            else:
                articles_to_process = all_articles
        else:
            # éç¬¬ä¸€é ï¼šè™•ç†æ‰€æœ‰æ–‡ç« 
            print(f"ğŸ“„ éç¬¬ä¸€é ï¼šè™•ç†æ‰€æœ‰æ–‡ç« ")
            articles_to_process = all_articles
            
        # åˆ†ææ–‡ç« ï¼Œæ”¶é›†URL
        article_urls = []
        old_articles_count = 0
        cutoff_date = datetime.datetime.now() - datetime.timedelta(days=target_days)
        
        for article in articles_to_process:
            try:
                # å–å¾—æ–‡ç« é€£çµ
                title_div = article.find('div', class_='title')
                if not title_div:
                    continue
                    
                link_element = title_div.find('a')
                if not link_element:
                    continue  # è·³éå·²åˆªé™¤æ–‡ç« 
                    
                article_url = 'https://www.ptt.cc' + link_element['href']
                article_title = link_element.get_text().strip()
                
                # å–å¾—æ–‡ç« æ—¥æœŸ
                date_div = article.find('div', class_='date')
                if not date_div:
                    continue
                    
                date_str = date_div.get_text().strip()
                
                # è§£ææ—¥æœŸ (æ ¼å¼: "12/29")
                try:
                    month, day = map(int, date_str.split('/'))
                    current_year = datetime.datetime.now().year
                    article_date = datetime.datetime(current_year, month, day)
                    
                    # å¦‚æœæ–‡ç« æ—¥æœŸåœ¨æœªä¾†ï¼Œè¡¨ç¤ºæ˜¯å»å¹´çš„æ–‡ç« 
                    if article_date > datetime.datetime.now():
                        article_date = article_date.replace(year=current_year - 1)
                        
                except (ValueError, IndexError):
                    print(f"âš ï¸ ç„¡æ³•è§£ææ—¥æœŸ: {date_str}")
                    continue
                    
                # æª¢æŸ¥æ–‡ç« æ˜¯å¦åœ¨ç›®æ¨™æ™‚é–“ç¯„åœå…§
                if article_date < cutoff_date:
                    old_articles_count += 1
                    print(f"â° ç™¼ç¾éèˆŠæ–‡ç« : {article_title[:30]}... ({date_str})")
                    continue
                    
                # æ”¶é›†ç¬¦åˆæ¢ä»¶çš„æ–‡ç« URL
                article_urls.append(article_url)
                
            except Exception as e:
                print(f"âš ï¸ è™•ç†æ–‡ç« æ™‚å‡ºéŒ¯: {e}")
                continue
                
        return {
            'article_urls': article_urls,
            'total_articles': len(all_articles),
            'skipped_pinned': skipped_pinned,
            'old_articles': old_articles_count,
            'should_stop': old_articles_count > 0  # å¦‚æœæœ‰éèˆŠæ–‡ç« ï¼Œå»ºè­°åœæ­¢
        }
        
    except Exception as e:
        print(f"âŒ åˆ†æé é¢ {page_number} æ™‚ç™¼ç”ŸéŒ¯èª¤: {e}")
        return None


if __name__ == "__main__":
    print("ğŸš€ PTT åˆ†æ•£å¼çˆ¬èŸ²ä»»å‹™ç™¼é€å™¨")
    print("=" * 60)
    print("æ¡ç”¨åˆ†æ•£å¼æ¶æ§‹ï¼šé€é åˆ†æï¼Œå¤š Worker ä¸¦è¡Œè™•ç†æ–‡ç« ")
    print("=" * 60)

    print("\nğŸš€ é–‹å§‹åŸ·è¡Œåˆ†æ•£å¼çˆ¬èŸ²...")
    # åŸ·è¡Œåˆ†æ•£å¼çˆ¬èŸ²ä»»å‹™
    result = send_distributed_crawl_task(
        board_name='Drink',
        target_days=30,    # çˆ¬å–è¿‘ 30 å¤©
        max_pages=5        # æ¸¬è©¦ç”¨ï¼Œæœ€å¤šè™•ç†5é 
    )

    if result['status'] == 'success':
        print("\nğŸ‰ åˆ†æ•£å¼çˆ¬èŸ²ä»»å‹™åˆ†ç™¼æˆåŠŸ!")
    else:
        print(f"\nâŒ åˆ†æ•£å¼çˆ¬èŸ²ä»»å‹™åˆ†ç™¼å¤±æ•—: {result.get('message', 'æœªçŸ¥éŒ¯èª¤')}")
